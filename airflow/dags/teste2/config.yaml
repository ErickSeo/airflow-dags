default:
  default_args:
    owner: "default_owner"
    start_date: 2024-11-06
    retries: 1
    retry_delay_sec: 300
  concurrency: 1
  max_active_runs: 1
  dagrun_timeout_sec: 600
  default_view: "graph"
  orientation: "LR"
  schedule_interval: "0 1 * * *"

teste:
    description: teste
    task_groups:
        tabela_1:
            tooltip: "teste"
    
    tasks:
        start_dag_render:
          operator: airflow.operators.dummy.DummyOperator 
          task_id: start_dag_render 

        upload_notebook_tabela_1:
          task_id: tabela_1_notebook
          task_group_name: tabela_1
          operator: utils.operators.databricks.upload_notebook.UploadDatabricksNotebook
          local_file_path: dags/repo/airflow/services/resources/dag_id/tabela_1.py
          remote_file_path: /test/resources/dag_id/tabela_1.py
          dependencies: [start_dag_render] 

        create_job_cluster_tabela_1:
          task_id: "create_job_cluster_tabela_1"
          task_group_name: tabela_1
          operator: utils.operators.databricks.create_job_cluster.DatabricksCreateJobCluster
          dependencies: [upload_notebook_tabela_1]
          notebook_task:
            notebook_path: /test/resources/dag_id/tabela_1.py
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            timeout_seconds: 36000
            gcp_attributes: 
              use_preemptible_executors: True
              google_service_account: "sa-allos-dbx-cluster-nonprd@allos-eng-dados-nonprd.iam.gserviceaccount.com"
              availability: "PREEMPTIBLE_WITH_FALLBACK_GCP"
              zone_id: "HA"
              local_ssd_count: 0
            node_type_id: "n2-standard-4"
            driver_node_type_id: "n2-standard-4"
            enable_elastic_disk: False
            autoscale:
              min_workers: 1
              max_workers: 3
            init_scripts: []
            spark_env_vars: 
              DAG_NAME: "{{ dag.dag_id }}"
              TASK_NAME: "{{ task.task_id }}"
              CRON_DATE: "{{ dag.schedule_interval }}"
              JOB_DATE: "{{ ds }}"
              JOB_DATETIME: "{{ execution_date }}"
              REFERENCE_DATE: "{{ next_ds }}"
              REFERENCE_DATETIME: "{{ next_execution_date }}"
              TABLE_NAME: "tabela_1"
              LAYER: "teste"
              ENVIRONMENT: "DEV"
            custom_tags:
              "tg_name_cluster": "dev_eng_shared_cluster"
              TaskName: "{{ task.task_id }}"
              DagName: "{{ dag.dag_id }}"
            spark_conf:
              "spark.app.name":  "{{ dag.dag_id }}"
              "spark.metrics.executorMetricsSource.enabled": False
              "spark.databricks.delta.formatCheck.enabled": False
              "spark.databricks.conda.condaMagic.enabled": True
              "spark.metrics.appStatusSource.enabled": True
            cluster_log_conf: 
              dbfs: 
                destination: "dbfs:/test/logs/"
          libraries: []
          access_control_list: []
          retries: 4
          retry_delay: 300
        
        wait_tabela_1_job_cluster:
          task_id: "wait_tabela_1_job_cluster"
          task_group_name: tabela_1
          dependencies: [create_job_cluster_tabela_1]
          operator: utils.sensors.databricks.job_sensor.DatabricksJobSensor
          run_id: "{{{{ task_instance.xcom_pull('{create_job_cluster_tabela_1}', key='run_id') }}}}"
          run_page_url: "{{{{ task_instance.xcom_pull('{create_job_cluster_tabela_1}', key='run_page_url') }}}}"
          retries: 3
          retry_delay: 300
